{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tamil\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import punctuation\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbols list that needs to be removed from sentences\n",
    "special_symbols = set(punctuation)\n",
    "special_symbols.remove(\"!\")\n",
    "special_symbols.remove(\",\")\n",
    "special_symbols.remove(\"?\")\n",
    "special_symbols.remove(\".\")\n",
    "special_symbols.add(\"”\")\n",
    "special_symbols.add(\"“\")\n",
    "special_symbols.add(\"‘\")\n",
    "special_symbols.add(\"’\")\n",
    "special_symbols.add(\"★\")\n",
    "special_symbols.add(\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence word length range\n",
    "MIN_WORDS_LENGTH = 3\n",
    "MAX_WORDS_LENGTH = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets path to the text file, cleans it according to rule and returns stats and valid sentences\n",
    "def get_commonvoice_sentences(text_file_path, work_title):\n",
    "    stats = {'work_title': work_title}\n",
    "    text = get_file_content(text_file_path)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Drop book's standard headers and footers\n",
    "    sentences = drop_header_and_footers(sentences)\n",
    "    stats['total_sentences_before_processing'] = len(sentences)\n",
    "    \n",
    "    valid_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Remove any words within brackets\n",
    "        sentence = re.sub('\\(.*?\\)','', sentence)\n",
    "        \n",
    "        # Remove extra white spaces\n",
    "        sentence = re.sub(\"\\s\\s+\", \" \", sentence)\n",
    "\n",
    "        # Remove special chracters\n",
    "        sentence_without_symbols = remove_special_characters(sentence, special_symbols)\n",
    "\n",
    "        # Drop sentences if they contain English characters\n",
    "        result =  bool(re.search(\"[a-zA-Z]\", sentence_without_symbols))\n",
    "        if result == True:\n",
    "            continue\n",
    "                \n",
    "        # Drop sentences if they contain number within a word\n",
    "        sentence_without_symbols = convert_num_to_tamil_string(sentence_without_symbols)\n",
    "        if sentence_without_symbols == False:\n",
    "            continue  \n",
    "        \n",
    "        # Drop too short and too long sentences\n",
    "        sentence_length = get_sentence_length_without_punctuation(sentence_without_symbols)\n",
    "        if (sentence_length > MAX_WORDS_LENGTH or sentence_length < MIN_WORDS_LENGTH):\n",
    "            continue\n",
    "            \n",
    "        sentence_without_symbols = clean_up_sentence(sentence_without_symbols)\n",
    "\n",
    "        sentence_dic = {\"work_title\": work_title, \"sentence\": sentence_without_symbols, \"sentence_length\": sentence_length}\n",
    "        valid_sentences.append(sentence_dic)\n",
    "    \n",
    "    # Drop duplicte sentences\n",
    "    valid_sentences_df = pd.DataFrame(valid_sentences)\n",
    "    valid_sentences_df = valid_sentences_df.drop_duplicates(subset='sentence', keep=\"first\")\n",
    "        \n",
    "    stats['total_sentences_after_processing'] = valid_sentences_df.shape[0]\n",
    "    return stats, valid_sentences_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the content of a text file\n",
    "def get_file_content(text_file_path):\n",
    "    text = \"\"\n",
    "    with open(text_file_path, 'r', encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_header_and_footers(sentences):\n",
    "    header_flag = False\n",
    "    footer_flag = False\n",
    "    \n",
    "    # Tamil Wikisource\n",
    "    book_content_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if \"உலகளாவிய பொதுக் கள உரிமம்\" in sentence: \n",
    "            header_flag = True\n",
    "        \n",
    "        if \"More details about this collaboration\" in sentence:  \n",
    "            header_flag = False\n",
    "        \n",
    "        if \"இந்த மின்னூலைப் பற்றி\" in sentence:  \n",
    "            footer_flag = True\n",
    "        \n",
    "        if header_flag == False and footer_flag == False:\n",
    "            book_content_sentences.append(sentence)\n",
    "    return book_content_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a sentence, it removes all symbols in the special_symbols list\n",
    "def remove_special_characters(sentence, special_symbols):\n",
    "    sentence = sentence.translate({ord(p): \" \" for p in special_symbols})\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a word in any sentence is a digit, it converts it to a tamil string\n",
    "# If there a digit within a word, it returns False\n",
    "def convert_num_to_tamil_string(sentence):\n",
    "    num_within_word = False\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word.strip().isdigit():\n",
    "            num_as_string = tamil.numeral.num2tamilstr_american(float(word))\n",
    "            num_as_string = re.sub(\"\\s\\s+\", \" \", num_as_string)\n",
    "            tokens[i] = num_as_string\n",
    "        else:\n",
    "            any_number = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n",
    "            if any_number.search(word) is not None:\n",
    "                num_within_word = True\n",
    "                break\n",
    "       \n",
    "    if num_within_word == True:\n",
    "        return False\n",
    "    else:\n",
    "        sentence = ' '.join([str(w) for w in tokens]) \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a sentence, calculates the word length without punctuation\n",
    "def get_sentence_length_without_punctuation(sentence):\n",
    "    sentence_without_punctuation = remove_special_characters(sentence, set(punctuation))\n",
    "    words_without_punctuation = word_tokenize(sentence_without_punctuation)\n",
    "    sentence_length = len(words_without_punctuation)\n",
    "    return sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces before punctuation\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence = sentence.replace(\" ,\", \",\")\n",
    "    sentence = sentence.replace(\" .\", \".\")\n",
    "    sentence = sentence.replace(\" ?\", \"?\")\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_texts = \"/home/nat/Desktop/code/tamil/open_tamil_texts/collections/tamil_wikisource/data\"\n",
    "extracted_sentences = \"cleaned_sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = glob.glob(source_texts + \"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_report = []\n",
    "for source_file in source_files:\n",
    "    base_name = os.path.basename(source_file)\n",
    "    work_title = base_name.replace(\".txt\", \"\")\n",
    "    print(\"processing \" + work_title)\n",
    "    result = get_commonvoice_sentences(source_file, work_title)\n",
    "    run_report.append(result[0])\n",
    "        \n",
    "    valid_sentences_df = result[1]\n",
    "    valid_sentences_df.to_csv(extracted_sentences + \"/\" + work_title + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_report_df = pd.DataFrame(run_report)\n",
    "run_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of sentences\n",
    "total_cv_sentences = run_report_df[\"total_sentences_after_processing\"].sum()\n",
    "print(\"Total common voice sentences: \" + str(total_cv_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage\n",
    "percent_converted_sentences = 100 * (run_report_df[\"total_sentences_after_processing\"].sum() / run_report_df[\"total_sentences_before_processing\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total common voice sentences as percentage of the original: \" + str(percent_converted_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_report_df.to_csv(\"tamil_wikisource_run_report.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
